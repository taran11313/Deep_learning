{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "To5_LLAUBjmh"
      },
      "source": [
        "\u003e Copyright 2022 DeepMind Technologies Limited.\n",
        "\u003e\n",
        "\u003e Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "\u003e you may not use this file except in compliance with the License.\n",
        "\u003e\n",
        "\u003e You may obtain a copy of the License at\n",
        "\u003e https://www.apache.org/licenses/LICENSE-2.0\n",
        "\u003e\n",
        "\u003e Unless required by applicable law or agreed to in writing, software\n",
        "\u003e distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "\u003e WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "\u003e See the License for the specific language governing permissions and\n",
        "\u003e limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUNjvbAiuiWQ"
      },
      "source": [
        "# Reinforcement Learning\n",
        "You may have already encountered **supervised learning**, where we have an input and a target value or class that we want to predict. There is also **unsupervised learning**, where we are only given an input and look for patterns in that input. In this practical, we look into **reinforcement learning** (RL), which can loosely be defined as training an **agent** to maximise a numerical **reward** it obtains through interaction with an **environment**.\n",
        "\n",
        "The environment defines a set of **actions** that an agent can take. The agent observes the current **state** of the environment, tries actions, and *learns* a **policy** which is a distribution over the possible actions given a state of the environment.\n",
        "\n",
        "The following diagram illustrates the interaction between the agent and the environment. We will explore each of the terms in more detail throughout this practical.\n",
        "\n",
        "\u003ccenter\u003e\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement-learning-summer-school/rl_agent_environment.png\" width=\"500\" /\u003e\u003c/center\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8AupLK3qmJ1"
      },
      "source": [
        "# Imports and dependencies.\n",
        "Please run the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lXxdlh0Y4Xhr"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies.\n",
        "%%capture\n",
        "\n",
        "!pip install bsuite\n",
        "!pip install dm-haiku\n",
        "!pip install optax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "dNFpWl9-4wVL"
      },
      "outputs": [],
      "source": [
        "#@title Imports.\n",
        "\n",
        "import collections\n",
        "import random\n",
        "from typing import Sequence\n",
        "\n",
        "import chex\n",
        "import dm_env\n",
        "from dm_env import specs\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from matplotlib import animation\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "\n",
        "rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJ9XAfT18BRh"
      },
      "source": [
        "## Catch Environment\n",
        "Let's begin by making a very simple game called \"Catch\", which is often used as a test bed for RL algorithms.\n",
        "\n",
        "In this environment, a ball drops from the top and the agent controls the paddle at the bottom via three possible  \u003cfont color='#0175c2'\u003e**Actions**\u003c/font\u003e: `left`, `stay`, and `right`. The \u003cfont color='#00ba47'\u003e**Reward**\u003c/font\u003e is given at the end of the episode, and is either `+1` for catching the ball or `-1` dropping the ball (reward is `0` in all intermediate steps). The episode ends when the ball reaches the bottom of the screen, and otherwise continues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xvY6DDgRTjLB"
      },
      "outputs": [],
      "source": [
        "#@title Defining the Catch Environment\n",
        "\n",
        "_ACTIONS = (-1, 0, 1)  # Move paddle left, no-op, move paddle right.\n",
        "\n",
        "class Catch(dm_env.Environment):\n",
        "  \"\"\"A Catch environment built on the dm_env.Environment class.\n",
        "\n",
        "  The agent must move a paddle to intercept falling balls. Falling balls only\n",
        "  move downwards on the column they are in.\n",
        "\n",
        "  The observation is an array with shape (rows, columns) containing binary\n",
        "  values: 0 if a space is empty; 1 if it contains the paddle or a ball.\n",
        "\n",
        "  The actions are discrete, and by default there are three available actions:\n",
        "  move left, stay, and move right.\n",
        "\n",
        "  The episode terminates when the ball reaches the bottom of the screen.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               rows: int = 10,\n",
        "               columns: int = 5,\n",
        "               discount: float = 1.0):\n",
        "    \"\"\"Initializes a new Catch environment.\n",
        "\n",
        "    Args:\n",
        "      rows: number of rows.\n",
        "      columns: number of columns.\n",
        "      discount: discount factor for calculating reward.\n",
        "    \"\"\"\n",
        "    self._rows = rows\n",
        "    self._columns = columns\n",
        "    self._discount = discount\n",
        "    self._board = np.zeros((rows, columns), dtype=np.float32)\n",
        "    self._ball_x = None\n",
        "    self._ball_y = None\n",
        "    self._paddle_x = None\n",
        "    self._reset_next_step = True\n",
        "\n",
        "  def reset(self) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Returns the first `TimeStep` of a new episode.\"\"\"\n",
        "    self._reset_next_step = False\n",
        "    # Ball can drop from any column.\n",
        "    self._ball_x = np.random.randint(self._columns)\n",
        "    self._ball_y = 0  # Top of matrix.\n",
        "    self._paddle_x = self._columns // 2  # Centre.\n",
        "\n",
        "    return dm_env.restart(self._observation())\n",
        "\n",
        "  def step(self, action: int) -\u003e dm_env.TimeStep:\n",
        "    \"\"\"Updates the environment according to the action.\"\"\"\n",
        "    if self._reset_next_step:\n",
        "      return self.reset()\n",
        "\n",
        "    # Move the paddle.\n",
        "    dx = _ACTIONS[action]  # Get action. dx = change in x position.\n",
        "    # Clip to keep paddle in bounds of the environment matrix.\n",
        "    self._paddle_x = np.clip(self._paddle_x + dx, 0, self._columns - 1)\n",
        "\n",
        "    # Drop the ball down one row.\n",
        "    self._ball_y += 1\n",
        "\n",
        "    # Check for termination.\n",
        "    if self._ball_y == self._rows - 1:  # Ball has fallen below the rows.\n",
        "      # Reward depends on whether the paddle is on the ball (positions match).\n",
        "      reward = 1. if self._paddle_x == self._ball_x else -1.\n",
        "      self._reset_next_step = True\n",
        "      return dm_env.termination(reward=reward, observation=self._observation())\n",
        "\n",
        "    return dm_env.transition(reward=0., observation=self._observation(),\n",
        "                             discount=self._discount)\n",
        "\n",
        "  def observation_spec(self) -\u003e specs.BoundedArray:\n",
        "    \"\"\"Returns the observation spec.\"\"\"\n",
        "    return specs.BoundedArray(\n",
        "        shape=self._board.shape,\n",
        "        dtype=self._board.dtype,\n",
        "        name='board',\n",
        "        minimum=0,\n",
        "        maximum=2)\n",
        "\n",
        "  def action_spec(self) -\u003e specs.DiscreteArray:\n",
        "    \"\"\"Returns the action spec.\"\"\"\n",
        "    return specs.DiscreteArray(\n",
        "        dtype=int, num_values=len(_ACTIONS), name='action')\n",
        "\n",
        "  def _observation(self) -\u003e np.ndarray:\n",
        "    self._board.fill(0.)\n",
        "    self._board[self._ball_y, self._ball_x] = 2.\n",
        "    self._board[self._rows - 1, self._paddle_x] = 1.\n",
        "\n",
        "    return self._board.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyBtSd6r5gfH"
      },
      "source": [
        "\n",
        "\n",
        "Before we start building an agent to interact with this environment, let's first look at the types of objects the environment returns (observations) and consumes (actions). The `environment_spec` will show you the form of the *observations*, *rewards* and *discounts* that the environment exposes and the form of the *actions* that can be taken:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CihHbsUW2-K8"
      },
      "outputs": [],
      "source": [
        "environment = Catch()\n",
        "print(environment.observation_spec())\n",
        "print(environment.reward_spec())\n",
        "print(environment.action_spec())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9clfe5Ok5qMP"
      },
      "source": [
        "We can see that by default the \u003cfont color='#ed005a'\u003e**observations**\u003c/font\u003e consist of a matrix of shape (10, 5). You can change the size of the game by setting the `rows` and `columns` parameters to different values. The \u003cfont color='#0175c2'\u003e**actions**\u003c/font\u003e is a 1-D integer array with possible values [0, 1, 2]. Finally, the \u003cfont color='#00ba47'\u003e**reward**\u003c/font\u003e is a scalar.\n",
        "\n",
        "Now we want to take an action using the `step` method to interact with the environment, which will return a `TimeStep` namedtuple with fields:\n",
        "\n",
        "```none\n",
        "step_type, reward, discount, observation\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5fVuABj4Jht"
      },
      "outputs": [],
      "source": [
        "# Reset and initialise the environment.\n",
        "step_type, reward, discount, observation = environment.reset()\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "\n",
        "# Let's take a single action.\n",
        "step_type, reward, discount, observation = environment.step(0)\n",
        "plt.imshow(observation)\n",
        "plt.show()\n",
        "\n",
        "print('\\nstep_type:', step_type)\n",
        "print('reward:', reward)\n",
        "print('discount:', discount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EgZ1M4296rz"
      },
      "source": [
        "`observation`: our observations are zero everywhere except where the paddle and ball are indicated by 1.\n",
        "\n",
        "`step_type`: indicates whether we're at the beginning, middle, or end of the episode. For more details, look [here](https://github.com/deepmind/dm_env/blob/master/dm_env/_environment.py#L32).\n",
        "\n",
        "`reward`: is the reward returned by the environment.\n",
        "\n",
        "`discount`: is the discount factor $\\gamma$. More details on what this is can be found in the *Agent-\u003eRun Loop* section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q042VZWQu2eu"
      },
      "source": [
        "## The Agent\n",
        "We now turn to the agent. An agent receives the current \u003cfont color='#ed005a'\u003e**state**\u003c/font\u003e and (previous) \u003cfont color='#00ba47'\u003e**reward**\u003c/font\u003e from the environment, then uses an internal policy to determine an \u003cfont color='#0175c2'\u003e**action**\u003c/font\u003e to take. We implement the agent as a Python [**class**](https://en.wikibooks.org/wiki/A_Beginner%27s_Python_Tutorial/Classes), which is just a logical wrapper of variables and methods (functions) that operate on those variables. The methods our agent will have are the following:\n",
        "\n",
        "\n",
        "* ```__init__```:  Initialises the agent the first time it's created.\n",
        "* `actor_step`: Receives the timestep information from the environment and returns an action.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGfwVqDrBMD5"
      },
      "source": [
        "# Random Agent\n",
        "\n",
        "To get a feel for an agent and the methods it has, let's first implement an agent that ignores the observations and just takes a *random* action at every step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByVhyZ8ovt1e"
      },
      "source": [
        "The main piece of information we need in order to implement this agent is the number of available actions in this environment. We can get this information from the `num_values` attribute of an action spec `env.action_spec()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "512FIVlo6AOb"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(object):\n",
        "  \"\"\"An agent which simply takes random actions.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray):\n",
        "    self._num_actions = action_spec.num_values\n",
        "\n",
        "  def actor_step(self, timestep: dm_env.TimeStep):\n",
        "    # This agent is ignoring the observations, so we delete timestep.\n",
        "    del timestep\n",
        "    # Return a random integer between 0 and (self._num_actions - 1).\n",
        "    return np.random.randint(self._num_actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYUk7zKy6pMP"
      },
      "source": [
        "### Run Loop\n",
        "\n",
        "Now we can loop through the environment using our random agent until the environment is terminated. We call each sequence of interactions with the environment until the termination of the episode.\n",
        "\n",
        "We also calculate the **episode return** at the end of an episode. The episode return is the sum of the (discounted) rewards obtained during the episode. If the reward for episode $i$ at time-step $t$ with trajectory $\\tau_{i}$ is denoted $\\color{#00ba47}{r_{i, t}}$, and the **discount factor** is $\\gamma$, then the episode return is calculated as:\n",
        "\n",
        "\n",
        "$$\\color{#00ba47}{r}(\\tau_i) = \\sum_{t=1}^{T_i} \\gamma^t \\color{#00ba47}{r_{i,t}}$$\n",
        "\n",
        "where $T_i$ is the episode length.\n",
        "\n",
        "The discount factor allows us to increase the importance of rewards received quickly and decrease the importance of rewards that take long to receive. It is especially important in environments that could have infinitely long episodes. In our particular environment where every episode is of the same length and the only non-zero reward is received at the end of the game, the discount factor doesn't make much difference and so we will ignore it (effectively set it to $1$) for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHrLqf7s0QnG"
      },
      "outputs": [],
      "source": [
        "num_episodes = 10\n",
        "\n",
        "# Initialise the environment.\n",
        "env = Catch()\n",
        "timestep = env.reset()\n",
        "\n",
        "# Initialise the agent.\n",
        "agent = RandomAgent(env.action_spec())\n",
        "\n",
        "# Run loop.\n",
        "for episode in range(num_episodes):\n",
        "  timesteps = []  # Accumulate data for the episode.\n",
        "\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "\n",
        "  while not timestep.last():\n",
        "    timesteps.append(timestep)\n",
        "    action = agent.actor_step(timestep)  # Acting.\n",
        "    timestep = env.step(action)  # Agent-environment interaction.\n",
        "\n",
        "  # Save the last timestep too.\n",
        "  timesteps.append(timestep)\n",
        "\n",
        "  # The first timestep is ignored due to having NaN as a reward.\n",
        "  returns = sum([item.reward for item in timesteps[1:]])\n",
        "  print(f'Episode {episode:2d}: Returns: {returns:.2f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQJ_1MQBq1bD"
      },
      "source": [
        "### Visualisation\n",
        "\n",
        "Let's build a function to animate the observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeSNG9bsq8eo"
      },
      "outputs": [],
      "source": [
        "def animate(data):\n",
        "  fig = plt.figure(1)\n",
        "  img = plt.imshow(data[0])\n",
        "  plt.axis('off')\n",
        "\n",
        "  def animate(i):\n",
        "    img.set_data(data[i])\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=len(timesteps))\n",
        "  plt.close(1)\n",
        "  return anim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uxip9hwrB76"
      },
      "source": [
        "We can now look at the game play for the last episode:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GejsyQM1q_d5"
      },
      "outputs": [],
      "source": [
        "animate([item.observation for item in timesteps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tC_9-rTz23pI"
      },
      "source": [
        "### **[Coding Task]**\n",
        "We assumed that $\\gamma= 1$ in the code above in the return calculation. Modify the code so that it computes discounts other than 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNTgaQwt344a"
      },
      "source": [
        "# Value-Based Reinforcement Learning\n",
        "\n",
        "Not surprisingly, our random agent is not really good at this game and we need to use some learning.\n",
        "\n",
        "\n",
        "In **value-based** reinforcement learning methods, agents maintain a value for all state-action pairs and use those estimates to choose actions that maximise that value (instead of maintaining a policy directly like policy gradient methods, which we will cover later).\n",
        "\n",
        "We represent the function mapping state-action pairs to values (otherwise known as a **Q-function**) for a specific policy $\\pi$ in a given [MDP](https://en.wikipedia.org/wiki/Markov_decision_process) as:\n",
        "\n",
        "$$ Q^{\\pi}(\\color{#ed005a}{s},\\color{#0175c2}{a}) = \\mathbb{E}_{\\tau \\sim P^{\\pi}} \\left[ \\sum_t \\gamma^t \\color{#00ba47}{R_t}| s_0=\\color{#ed005a}s,a=\\color{#0175c2}{a_0} \\right]$$\n",
        "\n",
        "where $\\tau = \\{\\color{#ed005a}{s_0}, \\color{#0175c2}{a_0}, \\color{#00ba47}{r_0}, \\color{#ed005a}{s_1}, \\color{#0175c2}{a_1}, \\color{#00ba47}{r_1}, \\cdots \\}$. In other words, $Q^{\\pi}(\\color{#ed005a}{s},\\color{#0175c2}{a})$ is the expected **value** (sum of discounted rewards) of being in a given \u003cfont color='#ed005a'\u003e**state**\u003c/font\u003e $\\color{#ed005a}s$ and taking the \u003cfont color='#0175c2'\u003e**action**\u003c/font\u003e $\\color{#0175c2}a$ and then following policy ${\\pi}$ thereafter.\n",
        "\n",
        "Efficient value estimations are based on the famous **_Bellman Optimality Equation_**:\n",
        "\n",
        "$$ Q^\\pi(\\color{#ed005a}{s},\\color{#0175c2}{a}) =  \\color{#00ba47}{r}(\\color{#ed005a}{s},\\color{#0175c2}{a}) + \\gamma  \\sum_{\\color{#ed005a}{s'}\\in \\color{#ed005a}{\\mathcal{S}}} P(\\color{#ed005a}{s'} |\\color{#ed005a}{s},\\color{#0175c2}{a}) V^\\pi(\\color{#ed005a}{s'}) $$\n",
        "\n",
        "which breaks down $Q^{\\pi}(\\color{#ed005a}{s},\\color{#0175c2}{a})$ into 2 parts: the immediate reward associated with being in state $\\color{#ed005a}{s}$ and taking action $\\color{#0175c2}{a}$, and the discounted sum of all future rewards. Note that $V^\\pi$ here is the expected $Q^\\pi$ value for a particular state, i.e.\n",
        "\n",
        "$$V^\\pi(\\color{#ed005a}{s}) = \\sum_{\\color{#0175c2}{a} \\in \\color{#0175c2}{\\mathcal{A}}} \\pi(\\color{#0175c2}{a} |\\color{#ed005a}{s}) Q^\\pi(\\color{#ed005a}{s},\\color{#0175c2}{a})$$\n",
        "\n",
        "**Note**: If you have not previously encountered Reinforcement Learning theory, these definitions might seem a bit dense! We recommend the following resources for learning about reinforcement learning:\n",
        "- [DeepMind x UCL RL Lecture Series](https://www.youtube.com/watch?v=TCCjZe0y4Qc)\n",
        "- [Introduction to Reinforcement Learning with David Silver](https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjbWFmgJznzp"
      },
      "source": [
        "## Q-learning Agent\n",
        "\n",
        "One of the simplest forms of value-based learning is [Q-learning](https://en.wikipedia.org/wiki/Q-learning). To implement this, we are going to change the random agent as follows:\n",
        "\n",
        "1. **Represent Q values.** We need to have a tabular representation of $Q$, which is a matrix of size `(number of states, number of actions)`. Our state space is the position of the ball and paddle in our grid so its size is $c*r*c$, where $r, c$ are the numbers of rows and columns, respectively. Our number of actions is 3 (move left, stay, move right).\n",
        "\n",
        "2. **Implement a policy.** For now, we are going to implement a greedy policy that returns the action with the highest $Q$ value. $$\\pi_{greedy} (\\color{#0175c2}a|\\color{#ed005a}s) = \\arg\\max_\\color{#0175c2}a Q^{\\pi_e}(\\color{#ed005a}s,\\color{#0175c2}a) $$\n",
        "\n",
        "3. **Implement a learning step.** We need to add a new method to our agent class to do the learning step, meaning update the $Q$ values based on a learning algorithm. We will call this new method  `learner_step`. The learning algorithm that we are going to use is called [temporal difference learning](https://en.wikipedia.org/wiki/Temporal_difference_learning). We will be updating our $Q$ value estimates at each step with the following update rule: \n",
        "\n",
        "$$Q(\\color{#ed005a}s, \\color{#0175c2}a) \\gets Q(\\color{#ed005a}s, \\color{#0175c2}a) + \\alpha \\delta$$\n",
        "\n",
        "The size of the (usually small) $\\alpha$ step size will influence how quickly our $Q$ values will be updated given new observations.\n",
        "\n",
        "The measure of error, the TD-error $\\delta$, is defined as:\n",
        "\n",
        "$$\\delta = \\color{#00ba47}R + \\gamma Q(\\color{#ed005a}{s'}, \\underbrace{\\pi_e(\\color{#ed005a}{s'}}_{\\color{#0175c2}{a'}})) − Q(\\color{#ed005a}s, \\color{#0175c2}a)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1pXuk_I0f7M"
      },
      "outputs": [],
      "source": [
        "class QlearningAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               step_size: float = 0.1):\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._step_size = step_size\n",
        "    r, c = observation_spec.shape\n",
        "    self._q = np.zeros((c * r * c, self._num_actions))\n",
        "\n",
        "  def _obs_to_index(self, obs):\n",
        "    \"\"\"Convert the observation into an index for accessing q values.\"\"\"\n",
        "    # The paddle location is always at the bottom.\n",
        "    obs_shape = obs.shape\n",
        "    paddle = np.where(obs[-1, :].flatten() == 1)[0][0]\n",
        "    obs = obs.flatten().astype(int)\n",
        "    # Case where the ball and paddle overlap.\n",
        "    if obs.sum() == 1:\n",
        "      ball = (obs_shape[0] - 1) * obs_shape[1]  + paddle\n",
        "    else:\n",
        "      ball = np.where(obs == 2)[0][0]\n",
        "    return paddle * np.prod(obs_shape) + ball\n",
        "\n",
        "  def actor_step(self, timestep):\n",
        "    # Index into the Q value matrix.\n",
        "    qvalue = self._q[self._obs_to_index(timestep.observation)]\n",
        "    # Greedy policy.\n",
        "    return np.argmax(qvalue)\n",
        "\n",
        "  def learner_step(self, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    # Offline Q-value update.\n",
        "    obs_t = self._obs_to_index(obs_t)\n",
        "    obs_tm1 = self._obs_to_index(obs_tm1)\n",
        "    # Greedy policy.\n",
        "    a_t = np.argmax(self._q[obs_t])\n",
        "    td_error = r_t + discount_t * self._q[obs_t, a_t] - self._q[obs_tm1, a_tm1]\n",
        "    self._q[obs_tm1, a_tm1] += self._step_size * td_error\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXfykgEK40J"
      },
      "source": [
        "To compute the loss, we need timestep information from both the current and last timesteps. We can achieve this by using local variables, but it would be cleaner code to define a new class to handle the data. We can call this new class `TransitionAccumulator`.\n",
        "\n",
        "At each timestep, we will save the data using the `push` method and retrieve data using the `sample` method. The `sample` method returns the previous observation in addition to the data for the current timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f7Lu3k88SBM"
      },
      "outputs": [],
      "source": [
        "Transition = collections.namedtuple(\n",
        "    'Transition', 'obs_tm1 a_tm1 r_t discount_t obs_t')\n",
        "\n",
        "# For now, this only handles batch_size=1 but you can modify the code\n",
        "# to handle other batch sizes.\n",
        "class TransitionAccumulator:\n",
        "  \"\"\"Simple Python accumulator for transitions.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self._prev = None\n",
        "    self._action = None\n",
        "    self._latest = None\n",
        "\n",
        "  def push(self, env_output, action):\n",
        "    self._prev = self._latest\n",
        "    self._action = action\n",
        "    self._latest = env_output\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    assert batch_size == 1\n",
        "    return Transition(self._prev.observation, self._action, self._latest.reward,\n",
        "                      self._latest.discount, self._latest.observation)\n",
        "\n",
        "  def is_ready(self, batch_size):\n",
        "    \"\"\"Checks if there is previous data stored.\"\"\"\n",
        "    assert batch_size == 1\n",
        "    return self._prev is not None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojuW1-ksMgQB"
      },
      "source": [
        "### Run Loop\n",
        "\n",
        "There is one final addition that we need to make to finish our run loop: a learning step.\n",
        "\n",
        "Since we are now training the agent using actual observations, we will need to run the environment for more episodes in order to gather sufficient data. We will only evaluate occasionally, every `evaluate_every` steps, to reduce the amount of logging info and computation (the latter point mostly applies later on when the agent's `actor_step` differs between train and evaluation time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIIbnC_C8W1A"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "train_episodes = 100\n",
        "evaluate_every = 10\n",
        "eval_episodes = 10\n",
        "seed = 1221\n",
        "\n",
        "# Initialise the environment.\n",
        "env = Catch()\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = QlearningAgent(env.action_spec(),\n",
        "                       env.observation_spec())\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = TransitionAccumulator()\n",
        "\n",
        "# Run loop\n",
        "avg_returns = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  accumulator.push(timestep, None)\n",
        "  while not timestep.last():\n",
        "    # Acting.\n",
        "    action = agent.actor_step(timestep)\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(action)\n",
        "    # Accumulate experience.\n",
        "    accumulator.push(timestep, action)\n",
        "    # Learning.\n",
        "    if accumulator.is_ready(batch_size):\n",
        "      agent.learner_step(*accumulator.sample(batch_size))\n",
        "   # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(timestep)\n",
        "        timestep = env.step(action)\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f'Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HzOGmN8uF54"
      },
      "source": [
        "Let's animate the last episode again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mft-sXXt4yy"
      },
      "outputs": [],
      "source": [
        "animate([item.observation for item in timesteps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktre7-oEjdwz"
      },
      "source": [
        "Hmmm, looks like the agent could use a bit more work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fllmzan8AqaX"
      },
      "source": [
        "### **[Coding Task]**\n",
        "A greedy policy with respect to a given estimate of $Q^\\pi$ fails to explore the environment as needed. An $\\epsilon$-greedy policy is a simple policy that at each time-step with probability $\\epsilon$ will choose a random action instead of the greedy action. Update the QlearningAgent's policy to an $\\epsilon$-greedy policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_hbC77hpAXHX"
      },
      "outputs": [],
      "source": [
        "class EGQlearningAgent(object):\n",
        "  \"\"\"Epsilon-greedy Q learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               epsilon: float = 0.1,\n",
        "               step_size: float = 0.1):\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._epsilon = epsilon\n",
        "    self._step_size = step_size\n",
        "    r, c = observation_spec.shape\n",
        "    self._q = np.zeros((c * r * c, self._num_actions))\n",
        "\n",
        "  def _obs_to_index(self, obs):\n",
        "    \"\"\"Convert the observation into an index for accessing q values.\"\"\"\n",
        "    # The paddle location is always at the bottom.\n",
        "    obs_shape = obs.shape\n",
        "    paddle = np.where(obs[-1, :].flatten() == 1)[0][0]\n",
        "    obs = obs.flatten().astype(int)\n",
        "    # Case where the ball and paddle overlap.\n",
        "    if obs.sum() == 1:\n",
        "      ball = (obs_shape[0] - 1) * obs_shape[1]  + paddle\n",
        "    else:\n",
        "      ball = np.where(obs == 2)[0][0]\n",
        "    return paddle * np.prod(obs_shape) + ball\n",
        "\n",
        "  def actor_step(self, timestep, evaluation):\n",
        "    # Index into the Q value matrix.\n",
        "    qvalue = self._q[self._obs_to_index(timestep.observation)]\n",
        "    # Epsilon-greedy policy.\n",
        "    if np.random.random() \u003e self._epsilon:\n",
        "      train_a = np.argmax(qvalue)\n",
        "    else:\n",
        "      train_a = np.random.choice(self._num_actions)\n",
        "    if evaluation:\n",
        "      return np.argmax(qvalue)\n",
        "    else:\n",
        "      return train_a\n",
        "\n",
        "  def learner_step(self, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    # Offline Q-value update.\n",
        "    obs_t = self._obs_to_index(obs_t)\n",
        "    obs_tm1 = self._obs_to_index(obs_tm1)\n",
        "    td_error = r_t + discount_t * np.max(self._q[obs_t]) - self._q[obs_tm1, a_tm1]\n",
        "    self._q[obs_tm1, a_tm1] += self._step_size * td_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Nou36XSZLb3h"
      },
      "outputs": [],
      "source": [
        "# @title **[Solution]** Epilson-greedy run-loop { form-width: \"30%\" }\n",
        "\n",
        "batch_size = 1\n",
        "train_episodes = 100\n",
        "evaluate_every = 10\n",
        "eval_episodes = 10\n",
        "seed = 1221\n",
        "\n",
        "# Initialise the environment.\n",
        "env = Catch(5, 2)  # Smaller environment to have smaller state.\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = EGQlearningAgent(env.action_spec(),\n",
        "                         env.observation_spec())\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = TransitionAccumulator()\n",
        "\n",
        "# Run loop.\n",
        "avg_returns = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  accumulator.push(timestep, None)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Acting.\n",
        "    action = agent.actor_step(timestep, False)\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(action)\n",
        "    # Accumulate experience.\n",
        "    accumulator.push(timestep, action)\n",
        "\n",
        "    # Learning.\n",
        "    if accumulator.is_ready(batch_size):\n",
        "      agent.learner_step(*accumulator.sample(batch_size))\n",
        "\n",
        "  # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(timestep, True)\n",
        "        timestep = env.step(action)\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHJs-b0TCWIk"
      },
      "source": [
        "### **[Coding Task]**\n",
        "\n",
        "Compare the result of greedy and epsilon greedy Q-Learning. Which one learns faster?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h0MUkhhztLe"
      },
      "source": [
        "# Deep Reinforcement Learning\n",
        "\n",
        "So far, we only considered look-up tables: in all the previous cases, every state and action pair $(\\color{#ed005a}{s}, \\color{#0175c2}{a})$, had an entry in our Q table. This is possible in this environment because the number of states is quite small. But this is not scalable to situations where, say, the goal location changes or the obstacles are in different locations at every episode (consider how big the table should be in this situation?).\n",
        "\n",
        "An example (not covered in this tutorial) is playing ATARI from pixels, where the number of possible frames an agent can see is exponential in the number of pixels on the screen.\n",
        "\n",
        "\u003ccenter\u003e\u003cimg width=\"200\" src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement-learning-summer-school/atari.gif\"\u003e\u003c/center\u003e\n",
        "\n",
        "But what we **really** want is just being able to *compute* the Q-value when fed with a particular $(\\color{#ed005a}{s}, \\color{#0175c2}{a})$ pair. So if we had a way to get a function to do this work instead of keeping a big table, we'd get around this problem.\n",
        "\n",
        "To address this, we can use **function approximation** as a way to generalise Q-values over some representation of a very large state space, and **train** our approximator to get it to output accurate Q-value estimates. In this section, we will explore Q-learning with function approximation, which, although theoretically proven to diverge for some degenerate MDPs, can yield impressive results in very large environments. [Playing Atari with Deep Reinforcement Learning](https://deepmind.com/research/publications/playing-atari-deep-reinforcement-learning)  introduced the first deep learning model to successfully learn control policies directly from high-dimensional pixel inputs using RL, and we're going to implement a simplified version of that agent here!\n",
        "\n",
        "\u003ccenter\u003e\u003cimg src=\"https://storage.googleapis.com/dm-educational/assets/reinforcement-learning-summer-school/dqn.jpeg\" width=\"500\" /\u003e\u003c/center\u003e\n",
        "\n",
        "We will predict $Q(\\color{#ed005a}s, \\color{#0175c2}a)$ using a neural network $f()$, which given a vector $\\color{#ed005a}s$, will output a vector of Q-values for all possible actions $\\color{#0175c2}a$.$^2$\n",
        "\n",
        "When using function approximations, particularly with neural networks, we need to have a loss to optimise. But looking back at the tabular setting above, you can see that we already have some notion of error: the **TD error**.\n",
        "\n",
        "By training our neural network to output values such that the *TD error is minimised*, we will also satisfy the Bellman Optimality Equation, which is a good sufficient condition to enforce so that we may obtain an optimal policy.\n",
        "Thanks to automatic differentiation, we can just write the TD error as a loss (e.g. with an $L2$ loss, but others would work too), compute its gradient (which are now gradients with respect to individual parameters of the neural network) and slowly improve our Q-value approximation:\n",
        "\n",
        "$$Loss = \\mathbb{E}\\left[ \\left( \\color{#00ba47}{r} + \\gamma \\max_\\color{#0175c2}{a'} Q(\\color{#ed005a}{s'}, \\color{#0175c2}{a'}) − Q(\\color{#ed005a}{s}, \\color{#0175c2}{a})  \\right)^2\\right]$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0xAdsgIzgCB"
      },
      "source": [
        "## Neural Net-Based Q-Learning Agent\n",
        "\n",
        "For our function approximator, we're going to use an [MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron) that takes the observation and outputs Q values for each of the actions. We construct the MLP inside the `__init__` function. We are going to use [Jax](https://github.com/google/jax) and [Haiku](https://github.com/deepmind/dm-haiku) to implement and train our neural nets. Please have a look [here](https://dm-haiku.readthedocs.io/en/latest/api.html) to understand Haiku transformations.\n",
        "\n",
        "One other class method we need to add to our agent is `initial_params`, which initialise the parameters of the neural network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NILUgTkF5gz-"
      },
      "outputs": [],
      "source": [
        "class QlearningAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               num_hiddens: Sequence[int] = [50],\n",
        "               epsilon: float = 0.01,\n",
        "               learning_rate: float = 0.005):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._epsilon = epsilon\n",
        "    self._optimizer = optax.adam(learning_rate)\n",
        "\n",
        "    def network(obs):\n",
        "      \"\"\"Q network of the agent.\"\"\"\n",
        "      flatten = lambda x: jnp.reshape(x, (-1,))\n",
        "      mlp = hk.Sequential(\n",
        "          [flatten,\n",
        "           hk.nets.MLP(num_hiddens + [self._num_actions])])\n",
        "      return mlp(obs)\n",
        "\n",
        "    self._network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "    # Jitting for speed.\n",
        "    self.actor_step = jax.jit(self.actor_step)\n",
        "    self.learner_step = jax.jit(self.learner_step)\n",
        "\n",
        "  def initial_params(self, rng_key):\n",
        "    \"\"\"Initialises the agent params given the RNG key.\"\"\"\n",
        "    sample_input = self._observation_spec.generate_value()\n",
        "    sample_input = jnp.expand_dims(sample_input, 0)\n",
        "    return self._network.init(rng_key, sample_input)\n",
        "\n",
        "  def initial_learner_state(self, params):\n",
        "    return self._optimizer.init(params)\n",
        "\n",
        "  def actor_step(self, params, timestep, rng_key, evaluation):\n",
        "    \"\"\"Given the observation, computes the action using epsilon-greedy algorithm.\"\"\"\n",
        "    qvalues = self._network.apply(params, timestep.observation)\n",
        "    if np.random.random() \u003e self._epsilon:\n",
        "      train_a = jnp.argmax(qvalues)\n",
        "    else:\n",
        "      train_a = jax.random.choice(rng_key, self._num_actions)\n",
        "\n",
        "    # If evaluating, return the greedy action. Otherwise, return the\n",
        "    # epsilon-greedy action.\n",
        "    return jax.lax.select(evaluation, jnp.argmax(qvalues), train_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKLbzqOrHAwB"
      },
      "source": [
        "Now we have an agent that uses an MLP to compute Q-values. But in its current state, the MLP params are just initialised randomly and not changed at all. We need to add a TD-Learning algorithm to our agent.\n",
        "\n",
        "`learner_step` will receive a collection of data that is collected from interacting with the environment using the `actor_step` function and then update the network parameters by computing the gradient of the loss function with respect to the network parameters.\n",
        "\n",
        "We also need to add an optimiser to the optimisation. We are going to use the [Adam optimizer](https://arxiv.org/abs/1412.6980), which is a very popular optimiser that requires less tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OKuW9XSI3pq"
      },
      "outputs": [],
      "source": [
        "class QlearningAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               num_hiddens: Sequence[int] = [50],\n",
        "               epsilon: float = 0.01,\n",
        "               learning_rate: float = 0.005):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._epsilon = epsilon\n",
        "    self._optimizer = optax.adam(learning_rate)\n",
        "\n",
        "    def network(obs):\n",
        "      \"\"\"Q network of the agent.\"\"\"\n",
        "      flatten = lambda x: jnp.reshape(x, (-1,))\n",
        "      mlp = hk.Sequential(\n",
        "          [flatten,\n",
        "           hk.nets.MLP(num_hiddens + [self._num_actions])])\n",
        "      return mlp(obs)\n",
        "\n",
        "    self._network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "    # Jitting for speed.\n",
        "    self.actor_step = jax.jit(self.actor_step)\n",
        "    self.learner_step = jax.jit(self.learner_step)\n",
        "\n",
        "  def initial_params(self, rng_key):\n",
        "    \"\"\"Initialises the agent params given the RNG key.\"\"\"\n",
        "    sample_input = self._observation_spec.generate_value()\n",
        "    sample_input = jnp.expand_dims(sample_input, 0)\n",
        "    return self._network.init(rng_key, sample_input)\n",
        "\n",
        "  def initial_learner_state(self, params):\n",
        "    return self._optimizer.init(params)\n",
        "\n",
        "  def actor_step(self, params, timestep, rng_key, evaluation):\n",
        "    \"\"\"Given the observation, computes the action using epsilon-greedy algorithm.\"\"\"\n",
        "    qvalues = self._network.apply(params, timestep.observation)\n",
        "    if np.random.random() \u003e self._epsilon:\n",
        "      train_a = jnp.argmax(qvalues)\n",
        "    else:\n",
        "      train_a = jax.random.choice(rng_key, self._num_actions)\n",
        "\n",
        "    # If evaluating, return the greedy action. Otherwise, return the\n",
        "    # epsilon-greedy action.\n",
        "    return jax.lax.select(evaluation, jnp.argmax(qvalues), train_a)\n",
        "\n",
        "  def learner_step(self, params: hk.Params, data, learner_state, rng_key):\n",
        "    \"\"\"Computes loss, its gradient w.r.t. params, and runs an optimisation step.\"\"\"\n",
        "    dloss_dtheta, loss = jax.grad(self._loss, has_aux=True)(params, *data)\n",
        "    updates, learner_state = self._optimizer.update(\n",
        "        dloss_dtheta, learner_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, learner_state, loss\n",
        "\n",
        "  def _loss(self, params, obs_tm1, a_tm1, r_t, discount_t, obs_t):\n",
        "    \"\"\"Computes the TD error loss.\"\"\"\n",
        "    q_tm1 = self._network.apply(params, obs_tm1)\n",
        "    q_t = self._network.apply(params, obs_t)\n",
        "\n",
        "    chex.assert_rank([q_tm1, a_tm1, r_t, discount_t, q_t], [1, 0, 0, 0, 1])\n",
        "    chex.assert_type([q_tm1, a_tm1, r_t, discount_t, q_t],\n",
        "                     [float, int, float, float, float])\n",
        "\n",
        "    target_tm1 = r_t + discount_t * jnp.max(q_t)\n",
        "    target_tm1 = jax.lax.stop_gradient(target_tm1)\n",
        "    td_error = target_tm1 - q_tm1[a_tm1]\n",
        "    loss = 0.5 * td_error ** 2\n",
        "    return loss, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGfv9JjS95Wx"
      },
      "source": [
        "### Run Loop\n",
        "Now we are ready to write the training loop. We're going to use the same accumulator from the previous section. Before starting the training, we need to initialise the agent's and optimiser's parameters.\n",
        "\n",
        "Similar to the tabular Q-learning agent, this agent also first acts in the environment and then accumulates the transition data inside an accumulator. Similarly, in `learner_step`, the agent does one step of learning and a parameter update:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PfsJwGC5l2o"
      },
      "outputs": [],
      "source": [
        "batch_size = 1\n",
        "train_episodes = 500\n",
        "evaluate_every = 50\n",
        "eval_episodes = 10\n",
        "seed = 1221\n",
        "\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
        "\n",
        "# Initialise the environment.\n",
        "env = Catch()\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = QlearningAgent(env.action_spec(),\n",
        "                       env.observation_spec())\n",
        "params = agent.initial_params(next(rng))\n",
        "learner_state = agent.initial_learner_state(params)\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = TransitionAccumulator()\n",
        "\n",
        "# Run loop.\n",
        "avg_returns = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  accumulator.push(timestep, None)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Acting.\n",
        "    action = agent.actor_step(params, timestep, next(rng), False)\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(action)\n",
        "    # Accumulate experience.\n",
        "    accumulator.push(timestep, action)\n",
        "\n",
        "    # Learning.\n",
        "    if accumulator.is_ready(batch_size):\n",
        "      params, learner_state, loss = agent.learner_step(\n",
        "          params, accumulator.sample(batch_size), learner_state, next(rng))\n",
        "      losses.append(np.asarray(loss))\n",
        "\n",
        "  # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(params, timestep, next(rng), True)\n",
        "        timestep = env.step(action)\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzYk75iuR9R7"
      },
      "source": [
        "Let's plot the loss and look at how it changes during training. But since we have a lot of data points, it's better to plot a moving average of the loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EORHNhChRscZ"
      },
      "outputs": [],
      "source": [
        "def moving_average(x, w):\n",
        "  return np.convolve(x, np.ones(w), 'valid') / w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBeZ3bZKRt2b"
      },
      "outputs": [],
      "source": [
        "plt.plot(moving_average(losses, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FsZe0LYSk0S"
      },
      "source": [
        "We can plot average returns during evaluations too:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjHZhxa5SpIJ"
      },
      "outputs": [],
      "source": [
        "plt.plot(avg_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKZJ99DnAUnJ"
      },
      "source": [
        "#### **[Coding Task]**\n",
        "At the moment, our Q-learning agent's learning only works with `batch_size=1`, which is very inefficient (can you think of why this is?). Try to update the agent and the accumulator so that the training can be done with a batch size larger than 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0osEU_5DmR2"
      },
      "source": [
        "## DQN Agent\n",
        "\n",
        "The agent we implemented above, while very successful on some tasks like [TD-Gammon](https://en.wikipedia.org/wiki/TD-Gammon), suffers from divergence issues and is hard to train for more complicated tasks.\n",
        "\n",
        "The [Deep Q-Learning Agent (DQN)](https://deepmind.com/research/publications/2019/playing-atari-deep-reinforcement-learning) improves on the Q-learning agent by incorporating two main ideas:\n",
        "\n",
        "*   `Replay buffer`: \"To alleviate the problems of correlated data and non-stationary distributions.\" [1]\n",
        "*   `Target network`: \"Use of an iterative update that adjusts the action-values (Q) towards target values that are only periodically updated, thereby reducing correlations with the target\" [1]\n",
        "\n",
        "First, let's make the replay buffer. We can modify our `TransitionAccumulator` slightly by adding a queue to collect more than one transition:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIEh7hUoDvTS"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, capacity, discount_factor=0.99):\n",
        "    self._discount_factor = discount_factor\n",
        "    self._prev = None\n",
        "    self._action = None\n",
        "    self._latest = None\n",
        "    self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "  def push(self, env_output, action):\n",
        "    self._prev = self._latest\n",
        "    self._action = action\n",
        "    self._latest = env_output\n",
        "\n",
        "    if action is not None:\n",
        "      self.buffer.append(\n",
        "          (self._prev.observation, self._action, self._latest.reward,\n",
        "           self._latest.discount, self._latest.observation))\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    obs_tm1, a_tm1, r_t, discount_t, obs_t = zip(\n",
        "        *random.sample(self.buffer, batch_size))\n",
        "    return (jnp.stack(obs_tm1), jnp.asarray(a_tm1), jnp.asarray(r_t),\n",
        "            jnp.asarray(discount_t) * self._discount_factor, jnp.stack(obs_t))\n",
        "\n",
        "  def is_ready(self, batch_size):\n",
        "    return batch_size \u003c= len(self.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nDDOxx5GBGQ"
      },
      "source": [
        "### The agent\n",
        "\n",
        "The second ingredient we need is a *target network*.\n",
        "\n",
        "At each iteration `i`, the target network computes the DQN loss $L_i$ on the parameters $\\theta_i$, based on a the set of target parameters $\\theta_{i-1}$ and a given batch of sampled trajectories `sample`. As described in the manuscript, the loss function is defined as:\n",
        "\n",
        "$$L_i (\\theta_i) = \\mathbb{E}_{\\color{#ed005a}{s},\\color{#0175c2}{a} \\sim \\rho(\\cdot)} \\left[ \\left( y_i - Q(\\color{#ed005a}{s},\\color{#0175c2}{a} ;\\theta_i) \\right)^2\\right]$$\n",
        "\n",
        "where the target $y_i$ is computed using a bootstrap value computed from Q-value network with target parameters:\n",
        "\n",
        "$$ y_i = \\mathbb{E}_{\\color{#ed005a}{s'} \\sim \\mathcal{E}} \\left[ \\color{#00ba47}{r} + \\gamma \\max_{\\color{#0175c2}{a'} \\in \\color{#0175c2}{\\mathcal{A}}} Q(\\color{#ed005a}{s'}, \\color{#0175c2}{a'} ; \\theta^{\\text{target}}_i) \\; | \\; \\color{#ed005a}{s}, \\color{#0175c2}{a} \\right] $$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcAP1h-vHW4f"
      },
      "outputs": [],
      "source": [
        "Params = collections.namedtuple(\"Params\", \"online target\")\n",
        "LearnerState = collections.namedtuple(\"LearnerState\", \"count opt_state\")\n",
        "\n",
        "class DQNAgent(object):\n",
        "  \"\"\"Q-learning agent.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               action_spec: specs.DiscreteArray,\n",
        "               observation_spec: specs.DiscreteArray,\n",
        "               num_hiddens: Sequence[int] = [50],\n",
        "               epsilon: float = 0.01,\n",
        "               learning_rate: float = 0.005,\n",
        "               target_period = 10):\n",
        "    self._observation_spec = observation_spec\n",
        "    self._num_actions = action_spec.num_values\n",
        "    self._epsilon = epsilon\n",
        "    self._target_period = target_period\n",
        "    self._optimizer = optax.adam(learning_rate)\n",
        "\n",
        "    def network(obs):\n",
        "      \"\"\"Q network of the agent.\"\"\"\n",
        "      # Unlike the previous version of the agent, here the observation has a\n",
        "      # leading batch dimension. Hence, we can use hk.Flatten(), which will\n",
        "      # flatten an array but leave the batch dimension intact.\n",
        "      mlp = hk.Sequential(\n",
        "          [hk.Flatten(),\n",
        "           hk.nets.MLP(num_hiddens + [self._num_actions])])\n",
        "      return mlp(obs)\n",
        "\n",
        "    self._network = hk.without_apply_rng(hk.transform(network, apply_rng=True))\n",
        "    # Jitting for speed.\n",
        "    self.actor_step = jax.jit(self.actor_step)\n",
        "    self.learner_step = jax.jit(self.learner_step)\n",
        "\n",
        "  def initial_params(self, rng_key):\n",
        "    \"\"\"Initialises the agent params given the RNG key.\"\"\"\n",
        "    sample_input = self._observation_spec.generate_value()\n",
        "    sample_input = jnp.expand_dims(sample_input, 0)\n",
        "    online_params = self._network.init(rng_key, sample_input)\n",
        "    return Params(online_params, online_params)\n",
        "\n",
        "  def initial_learner_state(self, params):\n",
        "    learner_count = jnp.zeros((), dtype=jnp.float32)\n",
        "    opt_state = self._optimizer.init(params.online)\n",
        "    return LearnerState(learner_count, opt_state)\n",
        "\n",
        "  def actor_step(self, params, timestep, rng_key, evaluation):\n",
        "    \"\"\"Given the observation, computes the action using epsilon-greedy algorithm.\"\"\"\n",
        "    # The actor step works with batch size 1 but our network expects\n",
        "    # the inputs to have a batch dimension.\n",
        "    obs = jnp.expand_dims(timestep.observation, 0)  # Add dummy batch.\n",
        "    qvalues = self._network.apply(params.online, obs)[0]  # Remove dummy batch.\n",
        "\n",
        "    if np.random.random() \u003e self._epsilon:\n",
        "      train_a = jnp.argmax(qvalues)\n",
        "    else:\n",
        "      train_a = jax.random.choice(rng_key, self._num_actions)\n",
        "\n",
        "    # If evaluating, return the greedy action. Otherwise, return the\n",
        "    # epsilon-greedy action.\n",
        "    return jax.lax.select(evaluation, jnp.argmax(qvalues), train_a)\n",
        "\n",
        "  def learner_step(self, params: hk.Params, data, learner_state, rng_key):\n",
        "    \"\"\"Computes the loss and its gradient with respect to the parameters and\n",
        "    does a step of optimisation.\"\"\"\n",
        "    # Update the target network parameters periodically.\n",
        "    is_time = learner_state.count % self._target_period == 0\n",
        "    target_params = jax.tree_map(\n",
        "        lambda new, old: jax.lax.select(is_time, new, old),\n",
        "        params.online, params.target)\n",
        "\n",
        "    dloss_dtheta, loss = jax.grad(self._loss, has_aux=True)(\n",
        "        params.online, target_params, *data)\n",
        "\n",
        "    updates, opt_state = self._optimizer.update(\n",
        "        dloss_dtheta, learner_state.opt_state)\n",
        "    online_params = optax.apply_updates(params.online, updates)\n",
        "    return (\n",
        "        Params(online_params, target_params),\n",
        "        LearnerState(learner_state.count + 1, opt_state),\n",
        "        loss)\n",
        "\n",
        "  def _loss(self, online_params, target_params, obs_tm1, a_tm1, r_t,\n",
        "            discount_t, obs_t):\n",
        "    \"\"\"Computes the TD error loss.\"\"\"\n",
        "    q_tm1 = self._network.apply(online_params, obs_tm1)\n",
        "    q_t_val = self._network.apply(target_params, obs_t)\n",
        "    q_t_select = self._network.apply(online_params, obs_t)\n",
        "\n",
        "    def q_learning_loss(q_tm1, a_tm1,  r_t, discount_t, q_t_value,\n",
        "                        q_t_selector):\n",
        "      target_tm1 = r_t + discount_t * q_t_value[q_t_selector.argmax()]\n",
        "      target_tm1 = jax.lax.stop_gradient(target_tm1)\n",
        "      return target_tm1 - q_tm1[a_tm1]\n",
        "\n",
        "    batched_loss = jax.vmap(q_learning_loss)\n",
        "    td_error = batched_loss(q_tm1, a_tm1, r_t, discount_t, q_t_val, q_t_select)\n",
        "    loss = jnp.mean(0.5 * td_error ** 2)\n",
        "    return loss, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBau9CdGtOU1"
      },
      "source": [
        "### Run Loop\n",
        "\n",
        "The training loop for the DQN agent is identical to the one we used above for the Q-learning agent. We only need to change the accumulator and the agent's class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhAn3GTde5uw"
      },
      "outputs": [],
      "source": [
        "batch_size = 10  #@param\n",
        "discount_factor = 0.99  #@param\n",
        "replay_buffer_capacity = 100  #@param\n",
        "train_episodes = 300  #@param\n",
        "evaluate_every = 25  #@param\n",
        "eval_episodes = 20  #@param\n",
        "seed = 1221  #@param\n",
        "\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(seed))\n",
        "\n",
        "# Initialise the environment.\n",
        "env = Catch()\n",
        "timestep = env.reset()\n",
        "\n",
        "# Build and initialise the agent.\n",
        "agent = DQNAgent(env.action_spec(),\n",
        "                 env.observation_spec())\n",
        "params = agent.initial_params(next(rng))\n",
        "learner_state = agent.initial_learner_state(params)\n",
        "\n",
        "# Initialise the accumulator.\n",
        "accumulator = ReplayBuffer(replay_buffer_capacity, discount_factor)\n",
        "\n",
        "# Run loop\n",
        "avg_returns = []\n",
        "losses = []\n",
        "\n",
        "for episode in range(train_episodes):\n",
        "  # Prepare agent, environment and accumulator for a new episode.\n",
        "  timestep = env.reset()\n",
        "  accumulator.push(timestep, None)\n",
        "\n",
        "  while not timestep.last():\n",
        "    # Acting.\n",
        "    action = agent.actor_step(params, timestep, next(rng), False)\n",
        "    # Agent-environment interaction.\n",
        "    timestep = env.step(action)\n",
        "    # Accumulate experience.\n",
        "    accumulator.push(timestep, action)\n",
        "    # Learning.\n",
        "    if accumulator.is_ready(batch_size):\n",
        "      params, learner_state, loss = agent.learner_step(\n",
        "          params, accumulator.sample(batch_size), learner_state, next(rng))\n",
        "      losses.append(np.asarray(loss))\n",
        "\n",
        "  # Evaluation.\n",
        "  if not episode % evaluate_every:\n",
        "    returns = []\n",
        "    for _ in range(eval_episodes):\n",
        "      timestep = env.reset()\n",
        "      timesteps = [timestep]\n",
        "      while not timestep.last():\n",
        "        action = agent.actor_step(params, timestep, next(rng), True)\n",
        "        timestep = env.step(action)\n",
        "        timesteps.append(timestep)\n",
        "      returns.append(np.sum([item.reward for item in timesteps[1:]]))\n",
        "\n",
        "    avg_returns.append(np.mean(returns))\n",
        "    print(f\"Episode {episode:4d}: Average returns: {avg_returns[-1]:.2f}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHgum6IWSYUE"
      },
      "outputs": [],
      "source": [
        "plt.plot(moving_average(losses, 50))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJVFfx_fxJBk"
      },
      "outputs": [],
      "source": [
        "animate([item.observation for item in timesteps])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWemzPgapZzs"
      },
      "source": [
        "That's looking like a much better ball-catching agent already! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vmhlx7lj2QQ"
      },
      "source": [
        "### **[Coding Task]**\n",
        "\n",
        "\n",
        "*   Collect loss and average returns for the whole training run and plot them.\n",
        "*   Play around with the parameters and observe their effect. A few suggestions:\n",
        "\u003e * Number of rows and columns for the game.\n",
        "\u003e * Number of hidden units and number of layers.\n",
        "\u003e * Learning rate.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJPnt4Mf_1aF"
      },
      "source": [
        "## Policy Gradients\n",
        "\n",
        "In this tutorial, we have looked at **value-based methods**. A popular alternative approach is using **policy gradient methods**.\n",
        "The name \"policy gradient\" comes from the fact that we are estimating the gradient of the policy, rather than the alternative, value-based RL such as Q-learning, which uses iterative update rules to calculate the expected return associated with a state and action.\n",
        "\n",
        "In order to learn, we need a loss function or *objective*. In RL, the general objective is to maximise the expected episode return (rewards) by taking actions in the environment. The actions our agent takes are determined by the policy $\\pi_\\theta(a|s)$, which is in turn determined by the neural network parameters $\\theta$. So, we want to find the neural network parameters $\\theta$ that maximise\n",
        "\n",
        "$$J(\\theta) = \\mathbb{E}_{\\tau}[r(\\tau)]$$\n",
        "\n",
        "Thanks for reading through this tutorial, we hope you enjoyed it and found it helpful for learning more about reinforcement learning! :)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBAGyOivNjAm"
      },
      "source": [
        "# References\n",
        "\n",
        "\n",
        "1.   Human-level control through deep reinforcement learning, V. Mnih *et al*., *Nature*, 2015. https://www.nature.com/articles/nature14236\n",
        "\n",
        "The materials in this colab are heavily borrowed from the following sources:\n",
        "\n",
        "1. [RLax](https://github.com/deepmind/rlax)\n",
        "2. [EEML2020 RL Tutorial](https://colab.research.google.com/github/eemlcommunity/PracticalSessions2020/blob/master/rl/EEML2020_RL_Tutorial.ipynb#scrollTo=acqPbd8zXH_K)\n",
        "3. [Indaba 2018](https://colab.sandbox.google.com/github/deep-learning-indaba/indaba-2018/blob/master/Practical_4_Reinforcement_Learning.ipynb#scrollTo=hQldYOWuu9RO)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Reinforcement Learning",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
